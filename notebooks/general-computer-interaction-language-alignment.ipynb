{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2be5e3d6",
   "metadata": {},
   "source": [
    "**Sincere apolagizes but this is definietly not worth reading**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a7af7ef-811c-417b-bf26-0ac0ef76fe9b",
   "metadata": {},
   "source": [
    "# General Computer Interaction Language Alignment Critic\n",
    "\n",
    "![](computerenv.png) *The general-purpose computer reasonably covers the anthropocentric problem domain. Performance across many tasks in this open-world domain therefore gives a proxy of development towards `artificial general intelligence'.*\n",
    "\n",
    "The general-purpose computer provides a simple interface to vast distributions of natural and synthetic complexity which reasonably proxy the anthropocentric problem domain. This inherently includes any dataset machine learning practitioners might use, billions hours of recorded audio and video, live social media feeds, uncountable scientific, engineering, business, and historical documents, as well as creative software, integrated development environments, simulators, engineering design tools, e-commerce platforms, business systems, and many more applications. Considered together with the Internet, the general-purpose computer is a ready-made multiagent, language-grounded, lifelong-learning environment-incubator for the development-evolution of progressively more capable, general, and autonomous artificial intelligence.\n",
    "\n",
    "Targeting this open set of tasks is not simple due to their non-stationary distribution. This is further complicated by heterogeneous user interfaces and context-sensitive application of natural world metaphors such as location, navigation, and gesture. Then there is also the issue of estimating task progress, completion, and reward in spite of shifting and overlapping task boundaries. While still keeping complete autonomy in mind as an ultimate objective, these challenges advocate occasionally relaxing the autonomy constraint in exchange for natural language human guidance.\n",
    "\n",
    "Natural language is already ubiquitous across graphical user interfaces. It allows transferring not only objectives but also cognitive models from human to agent thus helping align both the agent's action and perception. Genuinely expressed natural language (not template statements) communicates deep relational hierarchies and dependencies. Most importantly, natural language is a high-bandwidth channel to rapidly infuse human-oracle information into the policy inference loop online. Rapid feedback accelerates the entire training loop iterating towards increasing capability, generality, and autonomy. Conversely, measuring a computer interaction agent's sustained alignment with natural language instructions over long trajectories may provide a reasonable proxy of development towards the illusion of artificial general intelligence. (See figure above)\n",
    "\n",
    "![](integratedarchitecture.png) *Overall architecture of a general-purpose computer interaction agent. (a) The language alignment critic (this work, colored blue) provides feedback to an online continually learning policy. (b) Goals are encoded are natural language statements at various levels of granularity such as ``type \\$12345.00'', ``enter values from the document in their corresponding fields'', and ``file these electronic faxes''.*\n",
    "\n",
    "This work represents one step in that direction. I introduce a plan for a heterogeneous multitask, multimodal semi-supervised dataset of recorded computer interactions -- the User Experience (UE) -- and discuss how to train a multilevel action recognition system -- the General Computer Interaction Language Alignment Critic (GCI-LAC). Future work will use this critic network to not only passively measure action-language alignment but also guide active inference (keystrokes and mouse actions) in a real computer environment. (See figure above)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8684c5c8-3d08-4350-830e-c0a4aaddffd0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62730cca-c93e-460c-8ec4-2f93e5416e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q hub\n",
    "!pip install -q wikipedia\n",
    "!python -m pip install -q git+https://github.com/pytube/pytube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b216d0cf-34a2-4250-9cff-8027ba713d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import itertools\n",
    "import functools\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "import pytube\n",
    "import hub\n",
    "import tensorflow as tf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f376d001-d5bc-4a4a-9e8f-71d373aa2f2d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## The User Experience\n",
    "\n",
    "Computer interaction demands an understanding of diverse modalities: mouse events, keystrokes, language, audio, image, and video. At this scale of complexity, it is not currently feasible to build a massive supervised mouse-keyboard-text-audio-image-video dataset. Even if such a dataset were available, it may be unproductive to build training loops that demand every modality to be present in an example. For example, in many computer applications, the audio modality is ignored. It would be memory and compute efficient to similarly skip audio-related processing in corresponding dataset examples. However, in other applications such as media players, audio is essential and other modalities such as the keyboard and mouse can instead be ignored. Regardless of the modalities involved, this work aims to estimate a similarity measure between their current state and a natural language goal description. To my knowledge, no single dataset combines information from all these diverse modalities. Therefore, in this section I introduce a heterogeneous multimodal semi/supervised conglomerate dataset of datasets: the User Experience (UE). \n",
    "\n",
    "The User Experience is ~~currently~~ will be composed of 3 datasets: COCO, spoken mnist, and a synthesized dataset of keystrokes and mouse events. Table \\ref{table:ue} provides details on each of these classes. This collection will be expanded in the future. Some datasets in UE provide full descriptions at multiple levels of granularity, others pair brief or static inputs with single descriptions, and a large number merely provide raw data. Datasets are not batched by default. Each example is structured as a dictionary with the keys `mouse`, `keyboard`, `screen`, `audio`, `description`. Not all keys are present in every dataset example. The \\verb|mouse| modality is encoded by a 6-dimensional 32-bit float-valued tensor `<x location, y location, movement down (-) / up (+), movement left (-) / right (+), left button down, right button down>`. The `keyboard` modality is encoded as a 256-dimensional Boolean-valued vector with control, alphanumeric, and symbolic characters following ASCII mapping. The `screen` modality is variable sized RGB tensor with 32-bit floating point values already normalized in $[0,1]$. The `audio` modality is encoded in a variable length 16kHz, 16bit normalized waveform with amplitude values in $[-1, 1]$. The `description` modality contains a concatenated string of natural language descriptions for the action, image, or audio that it is paired with. If there are no descriptions, this modality is dropped (i.e.: it always has a nonzero length). Sample rate varies between datasets. However all modalities except for `audio` share a common number of timesteps per individual dataset example. The audio modality, if present, will have significantly more entries on its time axis as a result of its 16kHz sampling rate."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "42b5ee39-ee0c-489a-89a7-3eacb67e75ad",
   "metadata": {},
   "source": [
    "### COCA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d3e8e0-43d6-48a2-be5a-e6d436f25008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf dataset helper\n",
    "# copied from https://www.tensorflow.org/tutorials/text/image_captioning#download_and_prepare_the_ms-coco_dataset\n",
    "\n",
    "# Download caption annotation files\n",
    "annotation_folder = '/annotations/'\n",
    "if not os.path.exists(os.path.abspath('.') + annotation_folder):\n",
    "  annotation_zip = tf.keras.utils.get_file('captions.zip',\n",
    "                                           cache_subdir=os.path.abspath('.'),\n",
    "                                           origin='http://images.cocodataset.org/annotations/annotations_trainval2014.zip',\n",
    "                                           extract=True)\n",
    "  annotation_file = os.path.dirname(annotation_zip)+'/annotations/captions_train2014.json'\n",
    "  os.remove(annotation_zip)\n",
    "\n",
    "# Download image files\n",
    "image_folder = '/train2014/'\n",
    "if not os.path.exists(os.path.abspath('.') + image_folder):\n",
    "  image_zip = tf.keras.utils.get_file('train2014.zip',\n",
    "                                      cache_subdir=os.path.abspath('.'),\n",
    "                                      origin='http://images.cocodataset.org/zips/train2014.zip',\n",
    "                                      extract=True)\n",
    "  PATH = os.path.dirname(image_zip) + image_folder\n",
    "  os.remove(image_zip)\n",
    "else:\n",
    "  PATH = os.path.abspath('.') + image_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f84110c-0153-43b4-90ae-88b81a891ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# further processing\n",
    "# copied from https://www.tensorflow.org/tutorials/text/image_captioning#download_and_prepare_the_ms-coco_dataset\n",
    "\n",
    "import json\n",
    "import collections\n",
    "from PIL import Image\n",
    "\n",
    "with open(annotation_file, 'r') as f:\n",
    "    annotations = json.load(f)\n",
    "    \n",
    "# Group all captions together having the same image ID.\n",
    "image_path_to_caption = collections.defaultdict(list)\n",
    "for val in annotations['annotations']:\n",
    "  caption = f\"<start> {val['caption']} <end>\"\n",
    "  image_path = PATH + 'COCO_train2014_' + '%012d.jpg' % (val['image_id'])\n",
    "  image_path_to_caption[image_path].append(caption)\n",
    "\n",
    "image_paths = list(image_path_to_caption.keys())\n",
    "random.shuffle(image_paths)\n",
    "\n",
    "# Select the first 6000 image_paths from the shuffled set.\n",
    "# Approximately each image id has 5 captions associated with it, so that will\n",
    "# lead to 30,000 examples.\n",
    "train_image_paths = image_paths[:6000]\n",
    "print(len(train_image_paths))\n",
    "\n",
    "train_captions = []\n",
    "img_name_vector = []\n",
    "\n",
    "for image_path in train_image_paths:\n",
    "  caption_list = image_path_to_caption[image_path]\n",
    "  train_captions.extend(caption_list)\n",
    "  img_name_vector.extend([image_path] * len(caption_list))\n",
    "\n",
    "print(train_captions[0])\n",
    "Image.open(img_name_vector[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a2145407-36c7-4f52-95e7-02d6ef5b2519",
   "metadata": {},
   "source": [
    "### Spoken Mnist\n",
    "\n",
    "This dataset pairs the numbers 0 through 9 with spoken audio. Labels are integers numbered 0-9 and audio is normalized in $[-1,1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65fa22f-4f06-40d9-a2ef-863d7b91918c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spoken_mnist = hub.load(\"hub://activeloop/spoken_mnist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f7362d-2f8d-40c4-bb94-b27af8e4f4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "spoken_mnist.labels[0].numpy(), spoken_mnist.audio[0].numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e91cbd2-3448-4991-a41e-0e7aeda128c7",
   "metadata": {},
   "source": [
    "### Synthetic computer data set\n",
    "\n",
    "This dataset is just a collection of synthetic data. It is currently not useful for data engineering, but still is useful for testing basic model errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8f8ca1-15e9-4b1b-85c9-964d2e34f99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8770beab-9592-4392-8391-5e3dd43a66bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = wikipedia.summary(\"language\")\n",
    "keystrokes = [ord(c) for c in text]\n",
    "mouse = tf.random.normal(shape=(len(text), 2))\n",
    "ds = # TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "347cb271-e37c-4471-8fd5-698c4d866bae",
   "metadata": {},
   "source": [
    "## General Computer Interaction Language Alignment Critic\n",
    "\n",
    "As shown in the first figure, general-purpose computer interaction sits at the nexus of numerous problem domains involving mouse event and keystroke analysis, natural language processing, object detection, action sequence segmentation, audio/video understanding, and control. Drawing on existing contributions, this work combines pretrained models for most modalities separately and only trains a relatively small recurrent-state attention-based joint embedding network. The dot product between the joint embedding produced from computer modalities and the task semantic embedding is used to train a language alignment critic in CLIP-fashion. The figure below presents a visual anatomy of this architecture.\n",
    "\n",
    "![](architecture.png) *The language alignment critic uses a diverse set of modalities to predict a nontrivial vector that aligns with a language description semantic vector. Architecture primarily follows heuristic design.*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9ed51567-6a84-46d7-a16a-6b3ec9033eb3",
   "metadata": {},
   "source": [
    "## Experiment and Future Work \n",
    "\n",
    "Unfortunately, we did not have time to perform experiments on this architecture. Future work will finish the data loader and test a simple classifier architecture."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c6c74b19-43f5-4304-ad62-940f5c3beb00",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- https://www.tensorflow.org/tutorials/text/image_captioning\n",
    "\n",
    "- https://cocodataset.org/#home\n",
    "\n",
    "- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
